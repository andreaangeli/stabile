<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Ire-toronto : IRE's Toronto Watchdog Workshop" />

    <link rel="stylesheet" type="text/css" media="screen" href="../stylesheets/stylesheet.css">

    <title>Ire-toronto</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/sarahcnyt/ire-toronto">View on GitHub</a>

          <h1 id="project_title">Ire-toronto</h1>
          <h2 id="project_tagline">IRE's Toronto Watchdog Workshop</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/sarahcnyt/ire-toronto/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/sarahcnyt/ire-toronto/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a name="ire-toronto" class="anchor" href="#ire-toronto"><span class="octicon octicon-link"></span></a>ire-toronto</h1>
<a href="http://sarahcnyt.github.io/ire-toronto/tutorials/scraping-chrome.pdf">Here</a> is the tutorial for the workshop. 

<p>Scraping may be the most valuable new skill you'll learn this year. There are several tools out there to help you, with more coming out all the time. But at some point, if you do this enough, you'll find that learning to program a little is a lot easier than trying to wrangle a tool into something it didn't anticipate. </p>

<h2>
<a name="whats-scraping-and-what-can-you-do-with-it" class="anchor" href="#whats-scraping-and-what-can-you-do-with-it"><span class="octicon octicon-link"></span></a>What's scraping, and what can you do with it?</h2>

<p>With the simplest of tools:</p>

<ul>
<li>Download all of the documents linked off of a page.</li>
<li>Simple scrape of a table created on one page, like inmates at the <a href="http://showmeboone.com/sheriff/jailresidents/jailresidents.asp">Boone County jail</a> or salaries of officials in <a href="http://www.fin.gov.on.ca/en/publications/salarydisclosure/pssd/orgs.php?organization=legislative">Ontario's legislature</a>
</li>
<li>Simple scrape of a more complex page, like a page of articles from the IRE website, <a href="http://www.ire.org/blog/extra-extra/">Extra Extra</a>, or Craigslist listings for <a href="http://toronto.en.craigslist.ca/apa/">housing</a> in Toronto</li>
</ul><p>With some common tools:</p>

<ul>
<li>Extract everything from a set of paginated results. </li>
<li>Dig into detail pages when you have to click on each link on a site to get more. </li>
<li>Simulate simple searches, like going through each possible date on the White House <a href="http://www.whitehouse.gov/schedule/complete/2013-11-04">public schedule</a>. </li>
</ul><p>What's going to be hard no matter what:</p>

<ul>
<li>Security systems that require Javascript and checking of session cookies.</li>
<li>Search pages that require parameters you can't know in advance</li>
<li>Popups</li>
</ul><p>Most of these can be finessed with enough programming, but your simple tools probably won't do it. </p>

<h2>
<a name="tools" class="anchor" href="#tools"><span class="octicon octicon-link"></span></a>Tools</h2>

<h5>
<a name="free-and-easy" class="anchor" href="#free-and-easy"><span class="octicon octicon-link"></span></a>Free and easy</h5>

<ul>
<li>
<a href="https://addons.mozilla.org/en-US/firefox/addon/downthemall/">DownloadThemAll!</a> for Firefox, <a href="https://chrome.google.com/webstore/detail/download-master/mcceagdollnkjlogmdckgjakjapmkdjf?hl=en-US">Download Master</a> for Chrome.  These will let you capture all of the files linked off a page, filtering for just the ones you want. They are great when an agency has put a whole bunch of PDFs online and you want them all in one folder so you can search them.</li>
<li>Some people just use Google Docs for scraping simple pages, but I find them limiting and the formulas just don't work that well. </li>
<li>
<a href="https://chrome.google.com/webstore/detail/scraper/mbigbapnjcgaffohmbkdlecaccepngjd?hl=en">Chrome Scraper</a> extension, for parsing almost anything you can see on a page. Very powerful.</li>
</ul><h5>
<a name="not-too-expensive-and-more-powerful" class="anchor" href="#not-too-expensive-and-more-powerful"><span class="octicon octicon-link"></span></a>Not too expensive and more powerful</h5>

<ul>
<li>Outwit Hub](<a href="http://www.outwit.com/">http://www.outwit.com/</a>),  US$60 - standalone software or Firefox extension that automates going through multipage results and digging into detail pages.</li>
<li>
<a href="http://www.heliumscraper.com/en/index.php?p=home">Helium Scraper</a>, US$99 for Windows only. More powerful for very complicated pages, but it helps to know a little Javascript if you need to walk through forms or guess addresses.</li>
</ul><p>Each of these have their own odd language, so there's a considerable learning curve. If you're spending a lot of time learning the tool, consider putting your effort instead into learning a programming language like Ruby or Python for scraping.</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Ire-toronto maintained by <a href="https://github.com/sarahcnyt">sarahcnyt</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
